{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c04d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from googletrans import Translator\n",
    "from konlpy.tag import Okt\n",
    "import platform\n",
    "import sqlite3\n",
    "from matplotlib import font_manager, rc \n",
    "import matplotlib.pyplot as plt\n",
    "from gnews import GNews\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e064f62c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 299)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:299\u001b[0;36m\u001b[0m\n\u001b[0;31m    except Exception as e:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "DB_FILE = \"google_news_monitoring.db\"\n",
    "\n",
    "os_name = platform.system()\n",
    "if os_name == 'Windows':\n",
    "    font_name = 'Malgun Gothic'\n",
    "elif os_name == 'Darwin': # macOS\n",
    "    font_name = 'AppleGothic'\n",
    "elif os_name == 'Linux':\n",
    "    font_name = 'NanumGothic'\n",
    "else:\n",
    "    font_name = None\n",
    "\n",
    "if font_name:\n",
    "    try:\n",
    "        rc('font', family=font_name)\n",
    "        print(f\"{os_name} OS에서 '{font_name}' 폰트를 설정했습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"'{font_name}' 폰트를 찾을 수 없습니다. 시각화 시 한글이 깨질 수 있습니다. 오류: {e}\")\n",
    "else:\n",
    "    print(\"지원되지 않는 OS입니다. 폰트 설정이 필요합니다.\")\n",
    "\n",
    "def get_google_news(query, max_results=50, period='7d'):\n",
    "\n",
    "    print(f\"Searching Google News for: '{query}' (period: {period})\")\n",
    "    \n",
    "    google_news = GNews(\n",
    "        language='ko', \n",
    "        country='KR', \n",
    "        period=period,\n",
    "        max_results=max_results,\n",
    "        exclude_websites=['youtube.com', 'facebook.com'] \n",
    "    )\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            articles = google_news.get_news(query)\n",
    "            print(f\"Found {len(articles)} articles (attempt {attempt + 1})\")\n",
    "            \n",
    "            if articles:\n",
    "                return articles\n",
    "            else:\n",
    "                if period == '7d' and attempt < max_retries - 1:\n",
    "                    print(\"7일 기간에서 결과가 없어 30일로 확장합니다...\")\n",
    "                    google_news.period = '30d'\n",
    "                elif period == '30d' and attempt < max_retries - 1:\n",
    "                    print(\"30일 기간에서 결과가 없어 전체 기간으로 확장합니다...\")\n",
    "                    google_news.period = None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (attempt + 1) * 2\n",
    "                print(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "            \n",
    "    print(\"모든 재시도 실패\")\n",
    "    return []\n",
    "\n",
    "def create_flexible_queries(base_query):\n",
    "\n",
    "    basic_queries = [\n",
    "        base_query,  \n",
    "        base_query.replace('\"', ''), \n",
    "    ]\n",
    "    \n",
    "    unhcr_synonyms = ['유엔난민기구', 'UNHCR', '유엔 난민기구', '유엔난민청']\n",
    "    \n",
    "    related_keywords = [\n",
    "        ['난민', '피난민', '이주민'],\n",
    "        ['지원', '원조', '구호', '도움'],\n",
    "        ['갈등', '분쟁', '위기', '전쟁'],\n",
    "        ['인도적', '인권', '보호']\n",
    "    ]\n",
    "    \n",
    "    queries = basic_queries.copy()\n",
    "    \n",
    "    for synonym in unhcr_synonyms:\n",
    "        if synonym.lower() in base_query.lower():\n",
    "            continue\n",
    "        queries.append(f\"{base_query} {synonym}\")\n",
    "        queries.append(f\"{synonym} {base_query}\")\n",
    "    \n",
    "    for keyword_group in related_keywords:\n",
    "        for keyword in keyword_group[:2]:\n",
    "            queries.append(f\"{base_query} {keyword}\")\n",
    "    \n",
    "    return list(set(queries)) \n",
    "\n",
    "def test_search_queries(base_query):\n",
    "\n",
    "    test_queries = create_flexible_queries(base_query)\n",
    "    \n",
    "    additional_queries = [\n",
    "        f'\"{base_query}\" 난민',\n",
    "        f'\"{base_query}\" 지원',\n",
    "        f'{base_query} 뉴스',\n",
    "        f'{base_query} 최신',\n",
    "        base_query.split()[0] if ' ' in base_query else base_query,  \n",
    "    ]\n",
    "    \n",
    "    test_queries.extend(additional_queries)\n",
    "    test_queries = list(set(test_queries)) \n",
    "    \n",
    "    print(f\"\\n=== Testing {len(test_queries)} different search patterns ===\")\n",
    "    results = {}\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        try:\n",
    "            print(f\"\\n[Test {i}/{len(test_queries)}] Query: '{query}'\")\n",
    "            articles = get_google_news(query, max_results=20)  \n",
    "            results[query] = len(articles)\n",
    "            \n",
    "            if articles:\n",
    "                print(f\"  -> Found {len(articles)} articles.\")\n",
    "                first_title = articles[0].get('title', '')\n",
    "                print(f\"  Example: {first_title[:60]}...\")\n",
    "            else:\n",
    "                print(f\"  -> No results found.\")\n",
    "                \n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            results[query] = 0\n",
    "            \n",
    "    print(\"\\n=== Search Test Summary ===\")\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    for query, count in sorted_results:\n",
    "        print(f\"'{query}': {count} articles\")\n",
    "    \n",
    "    return dict(sorted_results)\n",
    "\n",
    "def init_db(db_path):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS articles (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            search_timestamp TEXT NOT NULL,\n",
    "            final_query TEXT NOT NULL,\n",
    "            title TEXT NOT NULL,\n",
    "            description TEXT,\n",
    "            link TEXT NOT NULL UNIQUE,\n",
    "            published_date TEXT,\n",
    "            sentiment_score REAL NOT NULL,\n",
    "            publisher TEXT\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"키워드 추출 함수 개선\"\"\"\n",
    "    try:\n",
    "        okt = Okt()\n",
    "        nouns = okt.nouns(text)\n",
    "        filtered_nouns = [n for n in nouns if len(n) > 1 and not n.isdigit()]\n",
    "        return filtered_nouns\n",
    "    except Exception as e:\n",
    "        print(f\"키워드 추출 중 오류: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_and_process_articles(articles, final_query, db_path):\n",
    "    translator = Translator()\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    total_compound_score = 0\n",
    "    article_count = 0\n",
    "    all_descriptions = \"\"\n",
    "    new_article_count = 0\n",
    "    \n",
    "    print(\"\\n--- Analyzing new articles and saving to database ---\")\n",
    "    \n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        for i, article in enumerate(articles, 1):\n",
    "            try: \n",
    "                link = article.get('url', '')\n",
    "                if not link: \n",
    "                    continue\n",
    "                    \n",
    "                cursor.execute(\"SELECT id FROM articles WHERE link = ?\", (link,))\n",
    "                if cursor.fetchone() is not None:\n",
    "                    continue\n",
    "                    \n",
    "                title = article.get('title', '')\n",
    "                description = article.get('description', '')\n",
    "                published_date = article.get('published date', '')\n",
    "                publisher = article.get('publisher', {}).get('title', '') if isinstance(article.get('publisher'), dict) else str(article.get('publisher', ''))\n",
    "                \n",
    "                if not title:\n",
    "                    continue\n",
    "                analysis_text = f\"{title} {description}\" if description else title\n",
    "                all_descriptions += analysis_text + \" \"\n",
    "                \n",
    "                try:\n",
    "                    translated_text = translator.translate(analysis_text[:500], src='ko', dest='en').text  # 길이 제한\n",
    "                    vs = analyzer.polarity_scores(translated_text)\n",
    "                    compound_score = vs['compound']\n",
    "                except Exception as trans_error:\n",
    "                    print(f\"  번역 오류, 원문으로 분석: {trans_error}\")\n",
    "                    vs = analyzer.polarity_scores(analysis_text)\n",
    "                    compound_score = vs['compound']\n",
    "                \n",
    "                print(f\" [{i}/{len(articles)}] (New) Title: {title[:50]}...\")\n",
    "                print(f\"   Sentiment Score: {compound_score:.4f}\")\n",
    "                \n",
    "                total_compound_score += compound_score\n",
    "                article_count += 1\n",
    "                new_article_count += 1\n",
    "                \n",
    "                now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO articles (search_timestamp, final_query, title, description, link, published_date, sentiment_score, publisher) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "                    (now, final_query, title, description, link, published_date, compound_score, publisher)\n",
    "                )\n",
    "                \n",
    "               \n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping article {i} due to error: {e}\")\n",
    "                continue\n",
    "                \n",
    "        conn.commit()\n",
    "                      \n",
    "    print(f\"\\n>> Saved {new_article_count} new articles to the database.\")\n",
    "    \n",
    "\n",
    "    if all_descriptions:\n",
    "        nouns = extract_keywords(all_descriptions)\n",
    "        top_keywords = Counter(nouns).most_common(10)\n",
    "    else:\n",
    "        top_keywords = []\n",
    "        \n",
    "    average_score = total_compound_score / article_count if article_count > 0 else 0\n",
    "                      \n",
    "    return average_score, top_keywords\n",
    "\n",
    "def visualize_top_keywords_sentiment(db_path):\n",
    "    \n",
    "    print(\"\\n[Keyword Sentiment Analysis] Analyzing all data in the DB to generate a graph...\")\n",
    "    try:\n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            df = pd.read_sql_query(\"SELECT title, description, sentiment_score FROM articles\", conn)\n",
    "                      \n",
    "        if df.empty:\n",
    "            print(\"No data in the database to analyze.\")\n",
    "            return\n",
    "dlt. plt . all_text = '' join df title df escription.fillnana        \n",
    "        all_text = ' '.join((df['title'] + ' ' + df['description'].fillna('')).tolist())\n",
    "        nouns = extract_keywords(all_text)\n",
    "        top_10_keywords = [keyword for keyword, count in Counter(nouns).most_common(10)]\n",
    "                      \n",
    "        if not top_10_keywords:\n",
    "            print(\"Could not find any keywords to analyze.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n>> Top 10 keywords for analysis: {', '.join(top_10_keywords)}\")\n",
    "        \n",
    "        keyword_sentiments = {}\n",
    "        for keyword in top_10_keywords:\n",
    "            mask = (df['title'].str.contains(keyword, case=False, na=False) | \n",
    "                   df['description'].str.contains(keyword, case=False, na=False))\n",
    "            avg_score = df[mask]['sentiment_score'].mean()\n",
    "            if pd.notna(avg_score):\n",
    "                keyword_sentiments[keyword] = avg_score\n",
    "        \n",
    "        if not keyword_sentiments:\n",
    "            print(\"키워드별 감성 분석 데이터를 찾을 수 없습니다.\")\n",
    "            return\n",
    "        if not keyword_sentiments\n",
    "        sorted_sentiments = sorted(keyword_sentiments.items(), key=lambda item: item[1], reverse=True)\n",
    "              \n",
    "        keywords = [item[0] for item in sorted_sentiments]\n",
    "        scores = [item[1] for item in sorted_sentiments]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.bar(keywords, scores, color='skyblue')\n",
    "        plt.axhline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "              \n",
    "        plt.title('Top 10 Keywords AVG Sentiment Score', fontsize=16)\n",
    "        plt.xlabel('Keyword', fontsize=12)\n",
    "        plt.ylabel('AVG Sentiment Score (Neg/Pos)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, axis='y', linestyle=':', alpha=0.6)\n",
    "              \n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.3f}', \n",
    "                    va='bottom' if yval >= 0 else 'top', ha='center')\n",
    "              \n",
    "        plt.tight_layout()\n",
    "        print(\"\\nDisplaying analysis graph. Close the graph window to continue.\")\n",
    "        plt.show()\n",
    "              \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the graph: {e}\")\n",
    "        \n",
    "def main():\n",
    "    init_db(DB_FILE)\n",
    "\n",
    "    base_query = input(\"검색할 기관/주제명을 입력하세요: \")\n",
    "\n",
    "    print(\"\\nTesting different search patterns to check for viability...\")\n",
    "    test_results = test_search_queries(base_query)\n",
    "\n",
    "   \n",
    "    valid_results = {q: c for q, c in test_results.items() if c > 0}\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(\"\\n 모든 검색어에서 결과를 찾지 못했습니다.\")\n",
    "        print(\"\\n 해결 방법:\")\n",
    "        print(\"1. 더 간단한 키워드로 시도해보세요\")\n",
    "        print(\"2. 영문 키워드를 시도해보세요\")\n",
    "        print(\"3. 네트워크 연결을 확인해주세요\")\n",
    "        print(\"4. 잠시 후 다시 시도해주세요\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n {len(valid_results)}개의 유효한 검색어를 찾았습니다!\")\n",
    "    \n",
    "    print(\"\\n[ 검색어 상세화 옵션 ]\")\n",
    "    print(\"1: 가장 많은 결과를 가진 검색어 사용\")\n",
    "    print(\"2: 기본 검색어만 사용\")\n",
    "    print(\"3: 수동으로 검색어 선택\")\n",
    "    \n",
    "    option = \"\"\n",
    "    while option not in ['1', '2', '3']:\n",
    "        option = input(\"원하는 옵션을 선택하세요: \")\n",
    "    \n",
    "    if option == '1':\n",
    "        best_query = max(valid_results, key=valid_results.get)\n",
    "        final_query = best_query\n",
    "        print(f\"\\n가장 많은 결과({valid_results[best_query]}개)를 가진 검색어를 사용합니다: '{best_query}'\")\n",
    "    elif option == '2':\n",
    "        final_query = base_query\n",
    "    else:  \n",
    "        print(\"\\n사용 가능한 검색어들:\")\n",
    "        query_list = list(valid_results.keys())\n",
    "        for i, (query, count) in enumerate(valid_results.items(), 1):\n",
    "            print(f\"{i}: '{query}' ({count}개 결과)\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                choice = int(input(f\"선택하세요 (1-{len(query_list)}): \")) - 1\n",
    "                if 0 <= choice < len(query_list):\n",
    "                    final_query = query_list[choice]\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"올바른 번호를 입력하세요.\")\n",
    "            except ValueError:\n",
    "                print(\"숫자를 입력하세요.\")\n",
    "\n",
    "    print(f\"\\n>> Final search query: '{final_query}'\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        print(\"\\nFinal search in progress...\")\n",
    "        news_articles = get_google_news(final_query, max_results=100)\n",
    "\n",
    "        if news_articles:\n",
    "            avg_sentiment, top_keywords = analyze_and_process_articles(news_articles, final_query, DB_FILE)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\" Summary of Newly Collected News Analysis \")\n",
    "            print(\"=\"*60)\n",
    "            print(f\" Articles found: {len(news_articles)}\")\n",
    "            print(f\" Average Sentiment Score: {avg_sentiment:.4f}\")\n",
    "            if avg_sentiment > 0.05:\n",
    "                print(\"  >> Overall sentiment appears to be 'Positive' \")\n",
    "            elif avg_sentiment < -0.05:\n",
    "                print(\"  >> Overall sentiment appears to be 'Negative' \")\n",
    "            else:\n",
    "                print(\"  >> Overall sentiment appears to be 'Neutral' \")\n",
    "\n",
    "            if top_keywords:\n",
    "                print(\"\\n Top 10 Keywords:\")\n",
    "                for keyword, count in top_keywords:\n",
    "                    print(f\"  - {keyword} ({count} times)\")\n",
    "\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\" All results have been cumulatively saved to '{DB_FILE}'.\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\n 최종 검색에서 결과를 찾지 못했습니다.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nA critical error occurred: {e}\")\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"\\nDB에 저장된 모든 데이터를 기반으로 키워드 감성 분석을 수행하시겠습니까? (y/n): \").lower()\n",
    "        if choice == 'y':\n",
    "            visualize_top_keywords_sentiment(DB_FILE)\n",
    "            break\n",
    "        elif choice == 'n':\n",
    "            print(\"프로그램을 종료합니다.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"잘못된 입력입니다. 'y' 또는 'n'을 입력해주세요.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a94d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
